{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JP9YXG2sc8Un",
        "outputId": "3cb2f5b5-865d-4fc9-af34-e0f945d6dd67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'DeepLearning_ComputerVision'...\n",
            "remote: Enumerating objects: 61, done.\u001b[K\n",
            "remote: Counting objects: 100% (61/61), done.\u001b[K\n",
            "remote: Compressing objects: 100% (52/52), done.\u001b[K\n",
            "remote: Total 61 (delta 9), reused 31 (delta 4), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (61/61), 7.28 MiB | 9.58 MiB/s, done.\n",
            "Resolving deltas: 100% (9/9), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/onkarkawade/DeepLearning_ComputerVision.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/DeepLearning_ComputerVision/projects/Transformer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XaY03N3JfjST",
        "outputId": "129b9137-3e12-4dd7-9749-6352a7f27329"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/DeepLearning_ComputerVision/projects/Transformer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from model import Transformer\n",
        "from transformers import AutoTokenizer  # pip install transformers\n",
        "from utils import (\n",
        "    BATCH_SIZE,\n",
        "    BLOCK_SIZE,\n",
        "    DEVICE,\n",
        "    DROPOUT,\n",
        "    LEARNING_RATE,\n",
        "    NUM_EMBED,\n",
        "    NUM_HEAD,\n",
        "    NUM_LAYER,\n",
        "    MAX_ITER,\n",
        "    EVAL_INTER,\n",
        "    encode,\n",
        "    decode,\n",
        "    get_batch,\n",
        "    save_model_to_chekpoint,\n",
        "    estimate_loss,\n",
        "    load_model_from_checkpoint\n",
        ")\n"
      ],
      "metadata": {
        "id": "IuOIXMIQf62k"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# raw data\n",
        "#path_do_data = \"data/english.txt\"\n",
        "path_do_data = \"data/Sample_project_report.txt\"\n",
        "data_raw = open(path_do_data, encoding=\"utf-8\").read()\n",
        "\n",
        "# we use pretrained BERT tokenizer for performance improvements\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "vocab_size = tokenizer.vocab_size\n",
        "\n"
      ],
      "metadata": {
        "id": "jnF5CS7hgHw_"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train/val split\n",
        "data = encode(text_seq=data_raw, tokenizer=tokenizer)\n",
        "n = int(0.9 * len(data))  # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R52Qf4_ogbpB",
        "outputId": "c0ef56c9-c609-49f6-a2a1-826e0c417e81"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (28377 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train a new model\n",
        "model = Transformer(\n",
        "    vocab_size=vocab_size,\n",
        "    num_embed=NUM_EMBED,\n",
        "    block_size=BLOCK_SIZE,\n",
        "    num_heads=NUM_HEAD,\n",
        "    num_layers=NUM_LAYER,\n",
        "    dropout=DROPOUT,\n",
        ")\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "TRR7yFuIhK9E",
        "outputId": "c9d4b582-4d61-4702-9b24-44a43c5618a1"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (token_embedding_table): Embedding(30522, 768)\n",
              "  (position_embedding_table): Embedding(64, 768)\n",
              "  (blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-5): 6 x AttentionHead(\n",
              "            (key): Linear(in_features=768, out_features=128, bias=False)\n",
              "            (query): Linear(in_features=768, out_features=128, bias=False)\n",
              "            (value): Linear(in_features=768, out_features=128, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-5): 6 x AttentionHead(\n",
              "            (key): Linear(in_features=768, out_features=128, bias=False)\n",
              "            (query): Linear(in_features=768, out_features=128, bias=False)\n",
              "            (value): Linear(in_features=768, out_features=128, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-5): 6 x AttentionHead(\n",
              "            (key): Linear(in_features=768, out_features=128, bias=False)\n",
              "            (query): Linear(in_features=768, out_features=128, bias=False)\n",
              "            (value): Linear(in_features=768, out_features=128, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-5): 6 x AttentionHead(\n",
              "            (key): Linear(in_features=768, out_features=128, bias=False)\n",
              "            (query): Linear(in_features=768, out_features=128, bias=False)\n",
              "            (value): Linear(in_features=768, out_features=128, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (4): TransformerBlock(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-5): 6 x AttentionHead(\n",
              "            (key): Linear(in_features=768, out_features=128, bias=False)\n",
              "            (query): Linear(in_features=768, out_features=128, bias=False)\n",
              "            (value): Linear(in_features=768, out_features=128, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (5): TransformerBlock(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-5): 6 x AttentionHead(\n",
              "            (key): Linear(in_features=768, out_features=128, bias=False)\n",
              "            (query): Linear(in_features=768, out_features=128, bias=False)\n",
              "            (value): Linear(in_features=768, out_features=128, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  (lm_head): Linear(in_features=768, out_features=30522, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load model to GPU if available\n",
        "m = model.to(DEVICE)\n",
        "# print the number of parameters in the model\n",
        "print(\n",
        "    \"Model with {:.2f}M parameters\".format(sum(p.numel() for p in m.parameters()) / 1e6)\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNR62SQQgcvU",
        "outputId": "0e419ea1-b65c-498c-d685-8e1b45e8ed0a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model with 89.48M parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# optimizer takes the model's parameters and the learning rate as input,\n",
        "# and updates the parameters during the training process in order to\n",
        "# minimize the loss function.\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=LEARNING_RATE)\n"
      ],
      "metadata": {
        "id": "fMVWU2vjhow9"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for step in range(MAX_ITER):\n",
        "\n",
        "    # every EVAL_INTER evaluate the loss on train and val sets\n",
        "    if step % EVAL_INTER == 0 or step == MAX_ITER - 1:\n",
        "        loss_train = estimate_loss(\n",
        "            data=train_data, model=m, block_size=BLOCK_SIZE, batch_size=BATCH_SIZE\n",
        "        )\n",
        "        loss_val = estimate_loss(\n",
        "            data=val_data, model=m, block_size=BLOCK_SIZE, batch_size=BATCH_SIZE\n",
        "        )\n",
        "        print(\"step {:10} | train loss {:6.4f} | val loss {:6.4f}\".format(step, loss_train, loss_val))\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch(data=train_data, block_size=BLOCK_SIZE, batch_size=BATCH_SIZE)\n",
        "    logits, loss = m.forward(xb, yb)\n",
        "    # zero_grad() method sets the gradients of all parameters in the optimizer to zero\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    # backward() method on the loss variable calculates the gradients\n",
        "    # of the loss with respect to the model's parameters.\n",
        "    loss.backward()\n",
        "    # step() method on the optimizer updates the model's parameters\n",
        "    # using the calculated gradients, in order to minimize the loss.\n",
        "    optimizer.step()\n",
        "\n",
        "save_model_to_chekpoint(model=m, path_to_checkpoint=\"checkpoints\", epoch=step)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1YGTGeSf-nK",
        "outputId": "67b5a78f-7a4d-469f-cb61-d07139b63623"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step          0 | train loss 10.7509 | val loss 10.7534\n",
            "step        500 | train loss 0.2771 | val loss 7.7868\n",
            "step       1000 | train loss 0.1336 | val loss 8.8329\n",
            "step       1500 | train loss 0.1146 | val loss 9.0112\n",
            "step       2000 | train loss 0.1169 | val loss 9.6346\n",
            "step       2500 | train loss 0.1143 | val loss 9.2942\n",
            "step       3000 | train loss 0.1047 | val loss 9.4503\n",
            "step       3500 | train loss 0.1047 | val loss 9.8033\n",
            "step       4000 | train loss 0.1009 | val loss 9.5543\n",
            "step       4500 | train loss 0.1061 | val loss 9.6153\n",
            "step       4999 | train loss 0.0958 | val loss 9.9655\n",
            "Successfully saved the model to checkpoints/checkpoint_epoch-4999_21.05.2025_06:38:03.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate some output based on the context\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=DEVICE)\n",
        "print(\n",
        "    decode(\n",
        "        enc_sec=m.generate(idx=context, max_new_tokens=50, block_size=BLOCK_SIZE)[0],\n",
        "        tokenizer=tokenizer,\n",
        "    )\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2todeaOhuR1",
        "outputId": "e4dfeecc-2e78-46b2-ba2e-f6a5ca5845ab"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PAD]. upon the foundation laid by r - cnn initially employed selective search ( uijlings, et al., 2013 ) as a region proposal technique to generate approximately 2000 region proposals per image. these proposals were then fed into a convolutional\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ask quetion\n",
        "\n",
        "model.to(DEVICE)\n",
        "# Define your prompt\n",
        "prompt = \"What is the this project about?\"\n",
        "\n",
        "# Tokenize prompt\n",
        "encoded_prompt = tokenizer.encode(prompt, return_tensors=\"pt\").to(DEVICE)\n",
        "\n",
        "with torch.no_grad():\n",
        "    output_ids = model.generate(idx=encoded_prompt, max_new_tokens=50, block_size=BLOCK_SIZE)\n",
        "answer = decode(enc_sec=output_ids[0], tokenizer=tokenizer)\n",
        "print(\"Answer:\", answer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzZS5rYGdmE5",
        "outputId": "6a3433c0-1919-491f-84e2-cd2615da5270"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: [CLS] who is the author for this project? [SEP] of the accuracy and quality of deep learning - based object detection. table 4. 1 : r - cnn series speed comparison ( kim, et al., 2020 ) 34 table 4. 2 : performance matrix of different scaled versions of yolo on coco\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from model import Transformer\n",
        "from utils import DEVICE, BLOCK_SIZE, decode,NUM_EMBED,NUM_HEAD,NUM_LAYER,DROPOUT\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Define your prompt\n",
        "#prompt = \"What is the capital of France?\"\n",
        "prompt = \"What is this project about?\"\n",
        "\n",
        "# Tokenize prompt\n",
        "encoded_prompt = tokenizer.encode(prompt, return_tensors=\"pt\").to(DEVICE)\n",
        "\n",
        "# Load model (same config as before)\n",
        "\n",
        "model = Transformer(\n",
        "    vocab_size=vocab_size,\n",
        "    num_embed=NUM_EMBED,\n",
        "    block_size=BLOCK_SIZE,\n",
        "    num_heads=NUM_HEAD,\n",
        "    num_layers=NUM_LAYER,\n",
        "    dropout=DROPOUT,\n",
        ")\n",
        "model\n",
        "model.load_state_dict(torch.load(\"checkpoints/checkpoint_epoch-4999_21.05.2025_06:38:03.pt\"))  # update path\n",
        "model.to(DEVICE)\n",
        "model.eval()\n",
        "\n",
        "# Generate answer\n",
        "with torch.no_grad():\n",
        "    output_ids = model.generate(idx=encoded_prompt, max_new_tokens=50, block_size=BLOCK_SIZE)\n",
        "\n",
        "# Decode and print answer\n",
        "answer = decode(enc_sec=output_ids[0], tokenizer=tokenizer)\n",
        "print(\"Answer:\", answer)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAgLVLOStdep",
        "outputId": "e5a364ec-7183-4b9c-d9a3-7f793e6ee5f2"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: [CLS] what is the capital of france? [SEP] are opportunities for further enhancement. future research could focus on leveraging specific helmet characteristics for improved detection against complex backgrounds and exploring combinations with additional vision algorithms for heightened precision. ( nath, et al., 2020 ) in his paper â€œ deep learning for\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FGVYw8XVlr-z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
